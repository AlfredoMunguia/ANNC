
@software{nasy_beamer_2022,
	title = {Beamer Theme in {SMILE}},
	url = {https://github.com/uta-smile/beamer-theme},
	publisher = {uta-smile},
	author = {Nasy},
	urldate = {2022-04-15},
	date = {2022-01-04},
	note = {original-date: 2021-11-01T19:06:32Z},
}

@article{zhang_bi-objective_2017,
	title = {Bi-objective workflow scheduling of the energy consumption and reliability in heterogeneous computing systems},
	volume = {379},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516305722},
	doi = {https://doi.org/10.1016/j.ins.2016.08.003},
	abstract = {Recent studies focus primarily on low energy consumption or execution time for task scheduling with precedence constraints in heterogeneous computing systems. In most cases, system reliability is more important than other performance metrics. In addition, energy consumption and system reliability are two conflicting objectives. A novel bi-objective genetic algorithm ({BOGA}) to pursue low energy consumption and high system reliability for workflow scheduling is presented in this paper. The proposed {BOGA} offers users more flexibility when jobs are submitted to a data center. On the basis of real-world and randomly generated application graphs, numerous experiments are conducted to evaluate the performance of the proposed algorithm. In comparison with excellent algorithms such as multi-objective heterogeneous earliest finish time ({MOHEFT}) and multi-objective differential evolution ({MODE}), {BOGA} performs significantly better in terms of finding the spread of compromise solutions.},
	pages = {241--256},
	journaltitle = {Information Sciences},
	author = {Zhang, Longxin and Li, Kenli and Li, Changyun and Li, Keqin},
	date = {2017},
	keywords = {Multi-objective, Precedence constraints, Reliability, Scheduling, Workflow},
}

@inproceedings{chilimbi_project_2014,
	location = {Broomfield, {CO}},
	title = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
	isbn = {978-1-931971-16-4},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
	pages = {571--582},
	booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
	publisher = {{USENIX} Association},
	author = {Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
	date = {2014-10},
}

@inproceedings{sankaradas_massively_2009,
	title = {A Massively Parallel Coprocessor for Convolutional Neural Networks},
	doi = {10.1109/ASAP.2009.25},
	pages = {53--60},
	booktitle = {2009 20th {IEEE} International Conference on Application-specific Systems, Architectures and Processors},
	author = {Sankaradas, Murugan and Jakkula, Venkata and Cadambi, Srihari and Chakradhar, Srimat and Durdanovic, Igor and Cosatto, Eric and Graf, Hans Peter},
	date = {2009},
}

@inproceedings{dean_large_2012,
	title = {Large Scale Distributed Deep Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc' aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	date = {2012},
}

@book{liu_parallelizing_2015,
	title = {Parallelizing Convolutional Neural Networks on Intel{\textasciicircum}\{{\textbackslash}textregistered \} Many Integrated Core Architecture},
	abstract = {Convolutional neural networks ({CNNs}) are state-of-the-art machine learning algorithm in low-resolution vision tasks and are widely applied in many applications. However, the training process of them is very time-consuming. As a result, many approaches have been proposed in which parallelization is one of the most effective. In this article, we parallelized a classic {CNN} on a new platform of Intel{\textbackslash}({\textasciicircum}\{\{{\textbackslash}textregistered \}\}{\textbackslash}) Xeon Phi{\textbackslash}({\textasciicircum}\{\{\{{\textbackslash}text \{{TM}\}\}\}\}{\textbackslash}) Coprocessor with {OpenMP}. Our implementation acquired 131{\textbackslash}({\textbackslash}times {\textbackslash}) speedup against the serial version running on the coprocessor itself and 8.3{\textbackslash}({\textbackslash}times {\textbackslash}) speedup against the serial baseline on the Xeon{\textbackslash}({\textasciicircum}\{\{{\textbackslash}textregistered \}\}{\textbackslash}) E5-2697 {CPU}.},
	pagetotal = {71},
	author = {Liu, Junjie and Wang, Haixia and Wang, Dongsheng and Gao, Yuan and Li, Zuofeng},
	date = {2015-03-24},
	doi = {10.1007/978-3-319-16086-3_6},
	note = {Pages: 82},
}

@inproceedings{ho_more_2013,
	location = {Red Hook, {NY}, {USA}},
	title = {More Effective Distributed {ML} via a Stale Synchronous Parallel Parameter Server},
	series = {{NIPS}'13},
	abstract = {We propose a parameter server system for distributed {ML}, which follows a Stale Synchronous Parallel ({SSP}) model of computation that maximizes the time computational workers spend doing useful work on {ML} algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an {ML} model's values (parameters and variables), and the {SSP} model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the {SSP} model ensures {ML} algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under {SSP}, as well as empirical results demonstrating that the {SSP} model achieves faster algorithm convergence on several different {ML} problems, compared to fully-synchronous and asynchronous schemes.},
	pages = {1223--1231},
	booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
	publisher = {Curran Associates Inc.},
	author = {Ho, Qirong and Cipar, James and Cui, Henggang and Kim, Jin Kyu and Lee, Seunghak and Gibbons, Phillip B. and Gibson, Garth A. and Ganger, Gregory R. and Xing, Eric P.},
	date = {2013},
	note = {event-place: Lake Tahoe, Nevada},
}

@inproceedings{song_c-brain_2016,
	title = {C-Brain: A deep learning accelerator that tames the diversity of {CNNs} through adaptive data-level parallelization},
	doi = {10.1145/2897937.2897995},
	pages = {1--6},
	booktitle = {2016 53nd {ACM}/{EDAC}/{IEEE} Design Automation Conference ({DAC})},
	author = {Song, Lili and Wang, Ying and Han, Yinhe and Zhao, Xin and Liu, Bosheng and Li, Xiaowei},
	date = {2016},
}

@online{noauthor_c-brain_nodate,
	title = {C-Brain: A deep learning accelerator that tames the diversity of {CNNs} through adaptive data-level parallelization {\textbar} {IEEE} Conference Publication {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/7544365},
	urldate = {2022-03-29},
}

@article{madura_t-cell_2013,
	title = {T-cell Receptor Specificity Maintained by Altered Thermodynamics},
	volume = {288},
	issn = {0021-9258},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3696650/},
	doi = {10.1074/jbc.M113.464560},
	abstract = {Background: The molecular principles governing T-cell specificity are poorly understood., Results: High affinity binding of a melanoma-specific T-cell receptor ({TCR}) is mediated through new {MHC} contacts and distinct thermodynamics., Conclusion: A novel thermodynamic mechanism upholds {TCR}-peptide specificity., Significance: {TCRs} can maintain peptide specificity using a mechanism that may enable widespread, safe enhancement of {TCR} binding affinity in therapeutic applications., The T-cell receptor ({TCR}) recognizes peptides bound to major histocompatibility molecules ({MHC}) and allows T-cells to interrogate the cellular proteome for internal anomalies from the cell surface. The {TCR} contacts both {MHC} and peptide in an interaction characterized by weak affinity ({KD} = 100 nm to 270 μm). We used phage-display to produce a melanoma-specific {TCR} (α24β17) with a 30,000-fold enhanced binding affinity ({KD} = 0.6 nm) to aid our exploration of the molecular mechanisms utilized to maintain peptide specificity. Remarkably, although the enhanced affinity was mediated primarily through new {TCR}-{MHC} contacts, α24β17 remained acutely sensitive to modifications at every position along the peptide backbone, mimicking the specificity of the wild type {TCR}. Thermodynamic analyses revealed an important role for solvation in directing peptide specificity. These findings advance our understanding of the molecular mechanisms that can govern the exquisite peptide specificity characteristic of {TCR} recognition.},
	pages = {18766--18775},
	number = {26},
	journaltitle = {The Journal of Biological Chemistry},
	shortjournal = {J Biol Chem},
	author = {Madura, Florian and Rizkallah, Pierre J. and Miles, Kim M. and Holland, Christopher J. and Bulek, Anna M. and Fuller, Anna and Schauenburg, Andrea J. A. and Miles, John J. and Liddy, Nathaniel and Sami, Malkit and Li, Yi and Hossain, Moushumi and Baker, Brian M. and Jakobsen, Bent K. and Sewell, Andrew K. and Cole, David K.},
	urldate = {2022-03-19},
	date = {2013-06-28},
	pmid = {23698002},
	pmcid = {PMC3696650},
}

@article{gee_antigen_2018,
	title = {Antigen Identification for Orphan T Cell Receptors Expressed on Tumor-Infiltrating Lymphocytes},
	volume = {172},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2017.11.043},
	abstract = {The immune system can mount T cell responses against tumors; however, the antigen specificities of tumor-infiltrating lymphocytes ({TILs}) are not well understood. We used yeast-display libraries of peptide-human leukocyte antigen ({pHLA}) to screen for antigens of "orphan" T cell receptors ({TCRs}) expressed on {TILs} from human colorectal adenocarcinoma. Four {TIL}-derived {TCRs} exhibited strong selection for peptides presented in a highly diverse {pHLA}-A∗02:01 library. Three of the {TIL} {TCRs} were specific for non-mutated self-antigens, two of which were present in separate patient tumors, and shared specificity for a non-mutated self-antigen derived from U2AF2. These results show that the exposed recognition surface of {MHC}-bound peptides accessible to the {TCR} contains sufficient structural information to enable the reconstruction of sequences of peptide targets for pathogenic {TCRs} of unknown specificity. This finding underscores the surprising specificity of {TCRs} for their cognate antigens and enables the facile indentification of tumor antigens through unbiased screening.},
	pages = {549--563.e16},
	number = {3},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Gee, Marvin H. and Han, Arnold and Lofgren, Shane M. and Beausang, John F. and Mendoza, Juan L. and Birnbaum, Michael E. and Bethune, Michael T. and Fischer, Suzanne and Yang, Xinbo and Gomez-Eerland, Raquel and Bingham, David B. and Sibener, Leah V. and Fernandes, Ricardo A. and Velasco, Andrew and Baltimore, David and Schumacher, Ton N. and Khatri, Purvesh and Quake, Stephen R. and Davis, Mark M. and Garcia, K. Christopher},
	date = {2018-01-25},
	pmid = {29275860},
	pmcid = {PMC5786495},
	keywords = {Adenocarcinoma, Aged, Animals, Antigens, Neoplasm, Cell Line, Tumor, Cells, Cultured, Colorectal Neoplasms, {HEK}293 Cells, {HLA}-A Antigens, Humans, Lymphocytes, Tumor-Infiltrating, Male, Middle Aged, Peptide Library, Receptors, Antigen, T-Cell, Sf9 Cells, Spodoptera, T cell, T cell receptor, antigens, cancer, combinatorial biology, human leukocyte antigen, ligand identification, peptide library, peptides, single-cell sequencing},
}

@article{tran_immunogenicity_2015,
	title = {Immunogenicity of somatic mutations in human gastrointestinal cancers},
	volume = {350},
	issn = {1095-9203},
	doi = {10.1126/science.aad1253},
	abstract = {It is unknown whether the human immune system frequently mounts a T cell response against mutations expressed by common epithelial cancers. Using a next-generation sequencing approach combined with high-throughput immunologic screening, we demonstrated that tumor-infiltrating lymphocytes ({TILs}) from 9 out of 10 patients with metastatic gastrointestinal cancers contained {CD}4(+) and/or {CD}8(+) T cells that recognized one to three neo-epitopes derived from somatic mutations expressed by the patient's own tumor. There were no immunogenic epitopes shared between these patients. However, we identified in one patient a human leukocyte antigen-C*08:02-restricted T cell receptor from {CD}8(+) {TILs} that targeted the {KRAS}(G12D) hotspot driver mutation found in many human cancers. Thus, a high frequency of patients with common gastrointestinal cancers harbor immunogenic mutations that can potentially be exploited for the development of highly personalized immunotherapies.},
	pages = {1387--1390},
	number = {6266},
	journaltitle = {Science (New York, N.Y.)},
	shortjournal = {Science},
	author = {Tran, Eric and Ahmadzadeh, Mojgan and Lu, Yong-Chen and Gros, Alena and Turcotte, Simon and Robbins, Paul F. and Gartner, Jared J. and Zheng, Zhili and Li, Yong F. and Ray, Satyajit and Wunderlich, John R. and Somerville, Robert P. and Rosenberg, Steven A.},
	date = {2015-12-11},
	pmid = {26516200},
	pmcid = {PMC7445892},
	keywords = {Adult, {CD}8-Positive T-Lymphocytes, Cell Line, Tumor, Female, Gastrointestinal Neoplasms, {HLA}-C Antigens, Humans, Immunodominant Epitopes, Immunotherapy, Lymphocytes, Tumor-Infiltrating, Male, Middle Aged, Mutation, Precision Medicine, Proto-Oncogene Proteins, Proto-Oncogene Proteins p21(ras), Receptors, Antigen, T-Cell, ras Proteins},
}

@article{cole_t-cell_2014,
	title = {T-cell Receptor ({TCR})-Peptide Specificity Overrides Affinity-enhancing {TCR}-Major Histocompatibility Complex Interactions},
	volume = {289},
	issn = {0021-9258},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3887192/},
	doi = {10.1074/jbc.M113.522110},
	abstract = {Background: {TCR} recognition of bipartite ligands composed of self ({MHC}) and non-self (peptide) maintains T-cell specificity., Results: Mutation of residues in the cognate peptide override {TCR} mutations that enhance {MHC} binding., Conclusion: {TCR}-{pMHC} binding affinity requires specific {TCR}-peptide interactions., Significance: Stabilization of {TCR}-{pMHC} engagement by {TCR}-peptide interactions maintains T-cell specificity and prevents recognition of self-{pMHC} in the periphery., αβ T-cell receptors ({TCRs}) engage antigens using complementarity-determining region ({CDR}) loops that are either germ line-encoded ({CDR}1 and {CDR}2) or somatically rearranged ({CDR}3). {TCR} ligands compose a presentation platform (major histocompatibility complex ({MHC})) and a variable antigenic component consisting of a short “foreign” peptide. The sequence of events when the {TCR} engages its peptide-{MHC} ({pMHC}) ligand remains unclear. Some studies suggest that the germ line elements of the {TCR} engage the {MHC} prior to peptide scanning, but this order of binding is difficult to reconcile with some {TCR}-{pMHC} structures. Here, we used {TCRs} that exhibited enhanced {pMHC} binding as a result of mutations in either {CDR}2 and/or {CDR}3 loops, that bound to the {MHC} or peptide, respectively, to dissect the roles of these loops in stabilizing {TCR}-{pMHC} interactions. Our data show that {TCR}-peptide interactions play a strongly dominant energetic role providing a binding mode that is both temporally and energetically complementary with a system requiring positive selection by self-{pMHC} in the thymus and rapid recognition of non-self-{pMHC} in the periphery.},
	pages = {628--638},
	number = {2},
	journaltitle = {The Journal of Biological Chemistry},
	shortjournal = {J Biol Chem},
	author = {Cole, David K. and Miles, Kim M. and Madura, Florian and Holland, Christopher J. and Schauenburg, Andrea J. A. and Godkin, Andrew J. and Bulek, Anna M. and Fuller, Anna and Akpovwa, Hephzibah J. E. and Pymm, Phillip G. and Liddy, Nathaniel and Sami, Malkit and Li, Yi and Rizkallah, Pierre J. and Jakobsen, Bent K. and Sewell, Andrew K.},
	urldate = {2022-03-19},
	date = {2014-01-10},
	pmid = {24196962},
	pmcid = {PMC3887192},
}

@article{liu_highly_2013,
	title = {Highly divergent T-cell receptor binding modes underlie specific recognition of a bulged viral peptide bound to a human leukocyte antigen class I molecule},
	volume = {288},
	issn = {1083-351X},
	doi = {10.1074/jbc.M112.447185},
	abstract = {Human leukocyte antigen ({HLA})-I molecules can present long peptides, yet the mechanisms by which T-cell receptors ({TCRs}) recognize featured {pHLA}-I landscapes are unclear. We compared the binding modes of three distinct human {TCRs}, {CA}5, {SB}27, and {SB}47, complexed with a "super-bulged" viral peptide ({LPEPLPQGQLTAY}) restricted by {HLA}-B*35:08. The {CA}5 and {SB}27 {TCRs} engaged {HLA}-B*35:08({LPEP}) similarly, straddling the central region of the peptide but making limited contacts with {HLA}-B*35:08. Remarkably, the {CA}5 {TCR} did not contact the α1-helix of {HLA}-B*35:08. Differences in the {CDR}3β loop between the {CA}5 and {SB}27 {TCRs} caused altered fine specificities. Surprisingly, the {SB}47 {TCR} engaged {HLA}-B*35:08({LPEP}) using a completely distinct binding mechanism, namely "bypassing" the bulged peptide and making extensive contacts with the extreme N-terminal end of {HLA}-B*35:08. This docking footprint included {HLA}-I residues not observed previously as {TCR} contact sites. The three {TCRs} exhibited differing patterns of alloreactivity toward closely related or distinct {HLA}-I allotypes. Thus, the human T-cell repertoire comprises a range of {TCRs} that can interact with "bulged" {pHLA}-I epitopes using unpredictable strategies, including the adoption of atypical footprints on the {MHC}-I.},
	pages = {15442--15454},
	number = {22},
	journaltitle = {The Journal of Biological Chemistry},
	shortjournal = {J Biol Chem},
	author = {Liu, Yu Chih and Miles, John J. and Neller, Michelle A. and Gostick, Emma and Price, David A. and Purcell, Anthony W. and {McCluskey}, James and Burrows, Scott R. and Rossjohn, Jamie and Gras, Stephanie},
	date = {2013-05-31},
	pmid = {23569211},
	pmcid = {PMC3668706},
	keywords = {{CD}8-Positive T-Lymphocytes, Complementarity Determining Regions, {HLA}-B35 Antigen, Herpesvirus 4, Human, Major Histocompatibility Complex ({MHC}), Peptides, Protein Structure, Secondary, Protein Structure, Tertiary, Receptors, Antigen, T-Cell, Structural Biology, T-cell Receptor, Viral Immunology, Viral Proteins, X-ray Crystallography},
}

@article{zhang_pird_2020,
	title = {{PIRD}: Pan Immune Repertoire Database},
	volume = {36},
	issn = {1367-4811},
	doi = {10.1093/bioinformatics/btz614},
	shorttitle = {{PIRD}},
	abstract = {{MOTIVATION}: T and B cell receptors ({TCRs} and {BCRs}) play a pivotal role in the adaptive immune system by recognizing an enormous variety of external and internal antigens. Understanding these receptors is critical for exploring the process of immunoreaction and exploiting potential applications in immunotherapy and antibody drug design. Although a large number of samples have had their {TCR} and {BCR} repertoires sequenced using high-throughput sequencing in recent years, very few databases have been constructed to store these kinds of data. To resolve this issue, we developed a database.
{RESULTS}: We developed a database, the Pan Immune Repertoire Database ({PIRD}), located in China National {GeneBank} ({CNGBdb}), to collect and store annotated {TCR} and {BCR} sequencing data, including from Homo sapiens and other species. In addition to data storage, {PIRD} also provides functions of data visualization and interactive online analysis. Additionally, a manually curated database of {TCRs} and {BCRs} targeting known antigens ({TBAdb}) was also deposited in {PIRD}.
{AVAILABILITY} {AND} {IMPLEMENTATION}: {PIRD} can be freely accessed at https://db.cngb.org/pird.},
	pages = {897--903},
	number = {3},
	journaltitle = {Bioinformatics (Oxford, England)},
	shortjournal = {Bioinformatics},
	author = {Zhang, Wei and Wang, Longlong and Liu, Ke and Wei, Xiaofeng and Yang, Kai and Du, Wensi and Wang, Shiyu and Guo, Nannan and Ma, Chuanchuan and Luo, Lihua and Wu, Jinghua and Lin, Liya and Yang, Fan and Gao, Fei and Wang, Xie and Li, Tao and Zhang, Ruifang and Saksena, Nitin K. and Yang, Huanming and Wang, Jian and Fang, Lin and Hou, Yong and Xu, Xun and Liu, Xiao},
	date = {2020-02-01},
	pmid = {31373607},
	keywords = {Antigens, Databases, Factual, High-Throughput Nucleotide Sequencing, Humans, Immunotherapy, Receptors, Antigen, T-Cell},
}

@article{bagaev_vdjdb_2020,
	title = {{VDJdb} in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium},
	volume = {48},
	issn = {1362-4962},
	doi = {10.1093/nar/gkz874},
	shorttitle = {{VDJdb} in 2019},
	abstract = {Here, we report an update of the {VDJdb} database with a substantial increase in the number of T-cell receptor ({TCR}) sequences and their cognate antigens. The update further provides a new database infrastructure featuring two additional analysis modes that facilitate database querying and real-world data analysis. The increased yield of {TCR} specificity identification methods and the overall increase in the number of studies in the field has allowed us to expand the database more than 5-fold. Furthermore, several new analysis methods are included. For example, batch annotation of {TCR} repertoire sequencing samples allows for annotating large datasets on-line. Using recently developed bioinformatic methods for {TCR} motif mining, we have built a reduced set of high-quality {TCR} motifs that can be used for both training {TCR} specificity predictors and matching against {TCRs} of interest. These additions enhance the versatility of the {VDJdb} in the task of exploring T-cell antigen specificities. The database is available at https://vdjdb.cdr3.net.},
	pages = {D1057--D1062},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Bagaev, Dmitry V. and Vroomans, Renske M. A. and Samir, Jerome and Stervbo, Ulrik and Rius, Cristina and Dolton, Garry and Greenshields-Watson, Alexander and Attaf, Meriem and Egorov, Evgeny S. and Zvyagin, Ivan V. and Babel, Nina and Cole, David K. and Godkin, Andrew J. and Sewell, Andrew K. and Kesmir, Can and Chudakov, Dmitriy M. and Luciani, Fabio and Shugay, Mikhail},
	date = {2020-01-08},
	pmid = {31588507},
	pmcid = {PMC6943061},
	keywords = {Amino Acid Sequence, Computational Biology, Databases, Genetic, High-Throughput Nucleotide Sequencing, Humans, Nucleotide Motifs, Position-Specific Scoring Matrices, Receptors, Antigen, T-Cell, Sequence Analysis, {DNA}, Software, V(D)J Recombination, Web Browser},
}

@article{joglekar_t_2019,
	title = {T cell antigen discovery via signaling and antigen-presenting bifunctional receptors},
	volume = {16},
	issn = {1548-7105},
	doi = {10.1038/s41592-018-0304-8},
	abstract = {{CD}8+ T cells recognize and eliminate tumors in an antigen-specific manner. Despite progress in characterizing the antitumor T cell repertoire and function, the identification of target antigens remains a challenge. Here we describe the use of chimeric receptors called signaling and antigen-presenting bifunctional receptors ({SABRs}) in a cell-based platform for T cell receptor ({TCR}) antigen discovery. {SABRs} present an extracellular complex comprising a peptide and major histocompatibility complex ({MHC}), and induce intracellular signaling via a {TCR}-like signal after binding with a cognate {TCR}. We devised a strategy for antigen discovery using {SABR} libraries to screen thousands of antigenic epitopes. We validated this platform by identifying the targets recognized by public {TCRs} of known specificities. Moreover, we extended this approach for personalized neoantigen discovery.},
	pages = {191--198},
	number = {2},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Joglekar, Alok V. and Leonard, Michael T. and Jeppson, John D. and Swift, Margaret and Li, Guideng and Wong, Stephanie and Peng, Songming and Zaretsky, Jesse M. and Heath, James R. and Ribas, Antoni and Bethune, Michael T. and Baltimore, David},
	date = {2019-02},
	pmid = {30700902},
	pmcid = {PMC6755906},
	keywords = {Antigen Presentation, Antigen-Presenting Cells, Antigens, Antigens, {CD}, Antigens, Differentiation, T-Lymphocyte, {CD}8-Positive T-Lymphocytes, Cloning, Molecular, Coculture Techniques, Epitopes, False Positive Reactions, Gene Library, Green Fluorescent Proteins, {HEK}293 Cells, Humans, Immunotherapy, Jurkat Cells, K562 Cells, Lectins, C-Type, Major Histocompatibility Complex, Oligonucleotides, Peptides, Receptors, Antigen, T-Cell, Signal Transduction},
}

@article{chen_sequence_2017,
	title = {Sequence and Structural Analyses Reveal Distinct and Highly Diverse Human {CD}8+ {TCR} Repertoires to Immunodominant Viral Antigens},
	volume = {19},
	issn = {2211-1247},
	doi = {10.1016/j.celrep.2017.03.072},
	abstract = {A diverse T cell receptor ({TCR}) repertoire is essential for controlling viral infections. However, information about {TCR} repertoires to defined viral antigens is limited. We performed a comprehensive analysis of {CD}8+ {TCR} repertoires for two dominant viral epitopes: pp65495-503 ({NLV}) of cytomegalovirus and M158-66 ({GIL}) of influenza A virus. The highly individualized repertoires (87-5,533 α or β clonotypes per subject) comprised thousands of unique {TCRα} and {TCRβ} sequences and dozens of distinct complementary determining region ({CDR})3α and {CDR}3β motifs. However, diversity is effectively restricted by preferential V-J combinations, {CDR}3 lengths, and {CDR}3α/{CDR}3β pairings. Structures of two {GIL}-specific {TCRs} bound to {GIL}-{HLA}-A2 provided a potential explanation for the lower diversity of {GIL}-specific versus {NLV}-specific repertoires. These anti-viral {TCRs} occupied up to 3.4\% of the {CD}8+ {TCRβ} repertoire, ensuring broad T cell responses to single epitopes. Our portrait of two anti-viral {TCR} repertoires may inform the development of predictors of immune protection.},
	pages = {569--583},
	number = {3},
	journaltitle = {Cell Reports},
	shortjournal = {Cell Rep},
	author = {Chen, Guobing and Yang, Xinbo and Ko, Annette and Sun, Xiaoping and Gao, Mingming and Zhang, Yongqing and Shi, Alvin and Mariuzza, Roy A. and Weng, Nan-Ping},
	date = {2017-04-18},
	pmid = {28423320},
	pmcid = {PMC5472051},
	keywords = {Adult, Amino Acid Motifs, Amino Acid Sequence, Antibody Affinity, Antigens, Viral, {CD}8 T cells, {CD}8-Positive T-Lymphocytes, Clone Cells, Complementarity Determining Regions, Consensus Sequence, Cytomegalovirus, {HLA}-A2 Antigen, Humans, Immunodominant Epitopes, Influenza A virus, Peptides, Protein Binding, Receptors, Antigen, T-Cell, Species Specificity, {TCR} repertoire, {TCR}-{pMHC} structure, human, αβ {TCRs} for {CMV}-{NLV}, αβ {TCRs} for {IAV}-{GIL}},
}

@article{huth_antigen-specific_2019,
	title = {Antigen-Specific {TCR} Signatures of Cytomegalovirus Infection},
	volume = {202},
	issn = {1550-6606},
	doi = {10.4049/jimmunol.1801401},
	abstract = {{CMV} is a prevalent human pathogen. The virus cannot be eliminated from the body, but is kept in check by {CMV}-specific T cells. Patients with an insufficient T cell response, such as transplant recipients, are at high risk of developing {CMV} disease. However, the {CMV}-specific T cell repertoire is complex, and it is not yet clear which T cells protect best against virus reactivation and disease. In this study, we present a highly resolved characterization of {CMV}-specific human {CD}8+ T cells based on enrichment by specific peptide stimulation and {mRNA} sequencing of their {TCR} β-chains ({TCRβ}). Our analysis included recently identified T cell epitopes restricted through {HLA}-C, whose presentation is resistant to viral immunomodulation, and well-studied {HLA}-B-restricted epitopes. In eight healthy virus carriers, we identified a total of 1052 {CMV}-specific {TCRβ} sequences. {HLA}-C-restricted, {CMV}-specific {TCRβ} clonotypes dominated the ex vivo T cell response and contributed the highest-frequency clonotype of the entire repertoire in two of eight donors. We analyzed sharing and similarity of {CMV}-specific {TCRβ} sequences and identified 63 public or related sequences belonging to 17 public {TCRβ} families. In our cohort, and in an independent cohort of 352 donors, the cumulative frequency of these public {TCRβ} family members was a highly discriminatory indicator of carrying both {CMV} infection and the relevant {HLA} type. Based on these findings, we propose {CMV}-specific {TCRβ} signatures as a biomarker for an antiviral T cell response to identify patients in need of treatment and to guide future development of immunotherapy.},
	pages = {979--990},
	number = {3},
	journaltitle = {Journal of Immunology (Baltimore, Md.: 1950)},
	shortjournal = {J Immunol},
	author = {Huth, Alina and Liang, Xiaoling and Krebs, Stefan and Blum, Helmut and Moosmann, Andreas},
	date = {2019-02-01},
	pmid = {30587531},
	keywords = {Antigens, Viral, {CD}8-Positive T-Lymphocytes, Cells, Cultured, Cytomegalovirus, Cytomegalovirus Infections, Epitopes, T-Lymphocyte, High-Throughput Nucleotide Sequencing, Humans, Peptides, Receptors, Antigen, T-Cell, alpha-beta, Transcriptome},
}

@online{noauthor_antigen-specific_nodate,
	title = {Antigen-Specific {TCR} Signatures of Cytomegalovirus Infection {\textbar} The Journal of Immunology},
	url = {https://www.jimmunol.org/content/202/3/979},
	urldate = {2022-03-19},
}

@article{glanville_identifying_2017,
	title = {Identifying specificity groups in the T cell receptor repertoire},
	volume = {547},
	rights = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature22976},
	doi = {10.1038/nature22976},
	abstract = {The authors devise an algorithm that can cluster T cell receptor ({TCR}) sequences sharing the same specificity, predict the {HLA} restriction of these {TCR} clusters on the basis of subjects’ genotypes and help to identify specific peptide major histocompatibility complex ligands.},
	pages = {94--98},
	number = {7661},
	journaltitle = {Nature},
	author = {Glanville, Jacob and Huang, Huang and Nau, Allison and Hatton, Olivia and Wagar, Lisa E. and Rubelt, Florian and Ji, Xuhuai and Han, Arnold and Krams, Sheri M. and Pettus, Christina and Haas, Nikhil and Arlehamn, Cecilia S. Lindestam and Sette, Alessandro and Boyd, Scott D. and Scriba, Thomas J. and Martinez, Olivia M. and Davis, Mark M.},
	urldate = {2022-03-19},
	date = {2017-07},
	langid = {english},
	note = {Number: 7661
Publisher: Nature Publishing Group},
	keywords = {Adaptive immunity, Protein sequence analyses, Statistical methods, Systems analysis, T-cell receptor},
}

@article{zhang_high-throughput_2018,
	title = {High-throughput determination of the antigen specificities of T cell receptors in single cells},
	volume = {36},
	rights = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.4282},
	doi = {10.1038/nbt.4282},
	abstract = {The antigen specificity of T cell receptors is measured at high throughput in single cells.},
	pages = {1156--1159},
	number = {12},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotechnol},
	author = {Zhang, Shu-Qi and Ma, Ke-Yue and Schonnesen, Alexandra A. and Zhang, Mingliang and He, Chenfeng and Sun, Eric and Williams, Chad M. and Jia, Weiping and Jiang, Ning},
	urldate = {2022-03-19},
	date = {2018-12},
	langid = {english},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Applied immunology, Immunotherapy, Molecular engineering, Next-generation sequencing, T-cell receptor},
}

@article{tickotsky_mcpas-tcr_2017,
	title = {{McPAS}-{TCR}: a manually curated catalogue of pathology-associated T cell receptor sequences},
	volume = {33},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btx286},
	doi = {10.1093/bioinformatics/btx286},
	abstract = {While growing numbers of T cell receptor ({TCR}) repertoires are being mapped by high-throughput sequencing, existing methods do not allow for computationally connecting a given {TCR} sequence to its target antigen, or relating it to a specific pathology. As an alternative, a manually-curated database can relate {TCR} sequences with their cognate antigens and associated pathologies based on published experimental data.We present {McPAS}-{TCR}, a manually curated database of {TCR} sequences associated with various pathologies and antigens based on published literature. Our database currently contains more than 5000 sequences of {TCRs} associated with various pathologic conditions (including pathogen infections, cancer and autoimmunity) and their respective antigens in humans and in mice. A web-based tool allows for searching the database based on different criteria, and for finding annotated sequences from the database in users’ data. The {McPAS}-{TCR} website assembles information from a large number of studies that is very hard to dissect otherwise. Initial analyses of the data provide interesting insights on pathology-associated {TCR} sequences.Free access at http://friedmanlab.weizmann.ac.il/{McPAS}-{TCR}/.},
	pages = {2924--2929},
	number = {18},
	journaltitle = {Bioinformatics},
	author = {Tickotsky, Nili and Sagiv, Tal and Prilusky, Jaime and Shifrut, Eric and Friedman, Nir},
	date = {2017-05},
	note = {\_eprint: https://academic.oup.com/bioinformatics/article-pdf/33/18/2924/25164209/btx286.pdf},
}

@online{noauthor_mcpas-tcr_nodate,
	title = {{McPAS}-{TCR}: a manually curated catalogue of pathology-associated T cell receptor sequences {\textbar} Bioinformatics {\textbar} Oxford Academic},
	url = {https://academic.oup.com/bioinformatics/article/33/18/2924/3803440?login=false},
	urldate = {2022-03-19},
}

@online{noauthor_iedborg_nodate,
	title = {{IEDB}.org: Free epitope database and prediction resource},
	url = {http://www.iedb.org},
	shorttitle = {{IEDB}.org},
	abstract = {Free resource for searching and exporting immune epitopes. Includes more than 95\% of all published infectious disease, allergy, autoimmune, and transplant epitope data.},
	urldate = {2022-03-19},
	langid = {english},
}

@article{chuang_debiased_2020,
	title = {Debiased contrastive learning},
	volume = {33},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
	date = {2020},
}

@online{noauthor_how_nodate,
	title = {how to import {BibTex} to Zotero},
	url = {https://forums.zotero.org/discussion/66009/how-to-import-bibtex-to-zotero},
	abstract = {Zotero is a powerful, easy-to-use research tool that helps you gather, organize, and analyze sources and then share the results of your research.},
	titleaddon = {Zotero Forums},
	urldate = {2022-03-17},
	langid = {english},
}

@inproceedings{saunshi_theoretical_2019,
	title = {A Theoretical Analysis of Contrastive Unsupervised Representation Learning},
	url = {https://proceedings.mlr.press/v97/saunshi19a.html},
	abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically “similar" data points and “negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.},
	eventtitle = {International Conference on Machine Learning},
	pages = {5628--5637},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
	urldate = {2022-03-17},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{robinson_contrastive_2020,
	title = {Contrastive Learning with Hard Negative Samples},
	url = {https://openreview.net/forum?id=CR1XOQ0UTh-},
	abstract = {We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard...},
	eventtitle = {International Conference on Learning Representations},
	author = {Robinson, Joshua David and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	urldate = {2022-03-17},
	date = {2020-09-28},
	langid = {english},
}

@article{karim_adversarial_2021,
	title = {Adversarial Training for Face Recognition Systems using Contrastive Adversarial Learning and Triplet Loss Fine-tuning},
	url = {http://arxiv.org/abs/2110.04459},
	abstract = {Though much work has been done in the domain of improving the adversarial robustness of facial recognition systems, a surprisingly small percentage of it has focused on self-supervised approaches. In this work, we present an approach that combines Ad-versarial Pre-Training with Triplet Loss {AdversarialFine}-Tuning. We compare our methods with the pre-trained {ResNet}50 model that forms the backbone of {FaceNet}, finetuned on our {CelebA} dataset. Through comparing adversarial robustness achieved without adversarial training, with triplet loss adversarial training, and our contrastive pre-training combined with triplet loss adversarial fine-tuning, we find that our method achieves comparable results with far fewer epochs re-quired during fine-tuning. This seems promising, increasing the training time for fine-tuning should yield even better results. In addition to this, a modified semi-supervised experiment was conducted, which demonstrated the improvement of contrastive adversarial training with the introduction of small amounts of labels.},
	journaltitle = {{arXiv}:2110.04459 [cs]},
	author = {Karim, Nazmul and Khalid, Umar and Meeker, Nick and Samarasinghe, Sarinda},
	urldate = {2022-03-17},
	date = {2021-10-09},
	eprinttype = {arxiv},
	eprint = {2110.04459},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{jiang_robust_2020,
	title = {Robust Pre-Training by Adversarial Contrastive Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/ba7e36c43aff315c00ec2b8625e3b719-Paper.pdf},
	pages = {16199--16210},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Jiang, Ziyu and Chen, Tianlong and Chen, Ting and Wang, Zhangyang},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	date = {2020},
}

@inproceedings{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classifier trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised {ResNet}-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100X fewer labels.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1597--1607},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2022-03-17},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{chen_aesptv_2020,
	title = {{aeSpTV}: An Adaptive and Efficient Framework for Sparse Tensor-Vector Product Kernel on a High-Performance Computing Platform},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2020.2990429},
	shorttitle = {{aeSpTV}},
	abstract = {Multi-dimensional, large-scale, and sparse data, which can be neatly represented by sparse tensors, are increasingly used in various applications such as data analysis and machine learning. A high-performance sparse tensor-vector product ({SpTV}), one of the most fundamental operations of processing sparse tensors, is necessary for improving efficiency of related applications. In this article, we propose {aeSpTV}, an adaptive and efficient {SpTV} framework on Sunway {TaihuLight} supercomputer, to solve several challenges of optimizing {SpTVon} high-performance computing platforms. First, to map {SpTV} to Sunway architecture and tame expensive memory access latency and parallel writing conflict due to the intrinsic irregularity of {SpTV}, we introduce an adaptive {SpTV} parallelization. Second, to co-execute with the parallelization design while still ensuring high efficiency, we design a sparse tensor data structure named {CSSoCR}. Third, based on the adaptive {SpTV} parallelization with the novel tensor data structure, we present an autotuner that chooses the most befitting tensor partitioning method for {aeSpTV} using the variance analysis theory of mathematical statistics to achieve load balance. Fourth, to further leverage the computing power of Sunway, we propose customized optimizations for {aeSpTV}. Experimental results show that {aeSpTV} yields good sacalability on both thread-level and process-level parallelism of Sunway. It achieves a maximum {GFLOPS} of 195.69 on 128 processes. Additionally, it is proved that optimization effects of the partitioning autotuner and optimization techniques are remarkable.},
	pages = {2329--2345},
	number = {10},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Chen, Yuedan and Xiao, Guoqing and Özsu, M. Tamer and Liu, Chubo and Zomaya, Albert Y. and Li, Tao},
	date = {2020-10},
	note = {Conference Name: {IEEE} Transactions on Parallel and Distributed Systems},
	keywords = {Data structures, Kernel, Optimization, Parallel, Parallel processing, Sparse matrices, Sunway architecture, Tensors, partition, sparse tensor data structure, sparse tensor-vector product},
}

@article{chen_bi-layered_2019,
	title = {A Bi-layered Parallel Training Architecture for Large-Scale Convolutional Neural Networks},
	volume = {30},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2018.2877359},
	abstract = {Benefitting from large-scale training datasets and the complex training network, Convolutional Neural Networks ({CNNs}) are widely applied in various fields with high accuracy. However, the training process of {CNNs} is very time-consuming, where large amounts of training samples and iterative operations are required to obtain high-quality weight parameters. In this paper, we focus on the time-consuming training process of large-scale {CNNs} and propose a Bi-layered Parallel Training ({BPT}-{CNN}) architecture in distributed computing environments. {BPT}-{CNN} consists of two main components: (a) an outer-layer parallel training for multiple {CNN} subnetworks on separate data subsets, and (b) an inner-layer parallel training for each subnetwork. In the outer-layer parallelism, we address critical issues of distributed and parallel computing, including data communication, synchronization, and workload balance. A heterogeneous-aware Incremental Data Partitioning and Allocation ({IDPA}) strategy is proposed, where large-scale training datasets are partitioned and allocated to the computing nodes in batches according to their computing power. To minimize the synchronization waiting during the global weight update process, an Asynchronous Global Weight Update ({AGWU}) strategy is proposed. In the inner-layer parallelism, we further accelerate the training process for each {CNN} subnetwork on each computer, where computation steps of convolutional layer and the local weight training are parallelized based on task-parallelism. We introduce task decomposition and scheduling strategies with the objectives of thread-level load balancing and minimum waiting time for critical paths. Extensive experimental results indicate that the proposed {BPT}-{CNN} effectively improves the training performance of {CNNs} while maintaining the accuracy.},
	pages = {965--976},
	number = {5},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Chen, Jianguo and Li, Kenli and Bilal, Kashif and zhou, xu and Li, Keqin and Yu, Philip S.},
	date = {2019-05},
	note = {Conference Name: {IEEE} Transactions on Parallel and Distributed Systems},
	keywords = {Acceleration, Big data, Computational modeling, Computer architecture, Distributed computing, Parallel processing, Task analysis, Training, bi-layered parallel computing, convolutional neural networks, deep learning, distributed computing},
}

@inproceedings{chen_hpspmv_2019,
	title = {{hpSpMV}: A Heterogeneous Parallel Computing Scheme for {SpMV} on the Sunway {TaihuLight} Supercomputer},
	doi = {10.1109/HPCC/SmartCity/DSS.2019.00142},
	shorttitle = {{hpSpMV}},
	abstract = {Sparse matrix-vector multiplication ({SpMV}) is one of the most essential algorithms in various applications. This paper designs {hpSpMV}, a heterogeneous parallel computing scheme for {SpMV}, on the Sunway {TaihuLight}. There are three main contributions of the {hpSpMV}. (1) We propose a heterogeneous parallelization design for the {SpMV} based on the heterogeneous manycore architecture of the {SW}26010 of Sunway {TaihuLight} and the given sparse matrix formats. (2) We analyze the execution time of the proposed heterogeneous parallel {SpMV} on the Sunway. (3) We propose an auto-tuning framework to set the proper parameter of the heterogeneous parallel {SpMV} based on the execution time analysis on the Sunway. We test the {hpSpMV}'s performance on the Sunway {TaihuLight}, the result analysis indicates that the {hpSpMV} has obvious performance improvement and good scalability on the Sunway {TaihuLight}.},
	eventtitle = {2019 {IEEE} 21st International Conference on High Performance Computing and Communications; {IEEE} 17th International Conference on Smart City; {IEEE} 5th International Conference on Data Science and Systems ({HPCC}/{SmartCity}/{DSS})},
	pages = {989--995},
	booktitle = {2019 {IEEE} 21st International Conference on High Performance Computing and Communications; {IEEE} 17th International Conference on Smart City; {IEEE} 5th International Conference on Data Science and Systems ({HPCC}/{SmartCity}/{DSS})},
	author = {Chen, Yuedan and Xiao, Guoqing and Xiao, Zheng and Yang, Wangdong},
	date = {2019-08},
	keywords = {Computer architecture, Conferences, Heterogeneous, sparse matrix format, sparse matrix-vector multiplication, parallel, Sunway {TaihuLight}, Kernel, Parallel processing, Program processors, Sparse matrices, Supercomputers},
}

@inproceedings{xie_applying_2019,
	title = {Applying Machine Learning to Understand Write Performance of Large-scale Parallel Filesystems},
	doi = {10.1109/PDSW49588.2019.00008},
	abstract = {In high-performance computing ({HPC}), I/O performance prediction offers the potential to improve the efficiency of scientific computing. In particular, accurate prediction can make runtime estimates more precise, guide users toward optimal checkpoint strategies, and better inform facility provisioning and scheduling policies. {HPC} I/O performance is notoriously difficult to predict and model, however, in large part because of inherent variability and a lack of transparency in the behaviors of constituent storage system components. In this work we seek to advance the state of the art in {HPC} I/O performance prediction by (1) modeling the mean performance to address high variability, (2) deriving model features from write patterns, system architecture and system configurations, and (3) employing Lasso regression model to improve model accuracy. We demonstrate the efficacy of our approach by applying it to a crucial subset of common {HPC} I/O motifs, namely, file-per-process checkpoint write workloads. We conduct experiments on two distinct production {HPC} platforms - Titan at the Oak Ridge Leadership Computing Facility and Cetus at the Argonne Leadership Computing Facility - to train and evaluate our models. We find that we can attain ≤ 30\% relative error for 92.79\% and 99.64\% of the samples in our test set on these platforms, respectively.},
	eventtitle = {2019 {IEEE}/{ACM} Fourth International Parallel Data Systems Workshop ({PDSW})},
	pages = {30--39},
	booktitle = {2019 {IEEE}/{ACM} Fourth International Parallel Data Systems Workshop ({PDSW})},
	author = {Xie, Bing and Tan, Zilong and Carns, Philip and Chase, Jeff and Harms, Kevin and Lofstead, Jay and Oral, Sarp and Vazhkudai, Sudharshan S. and Wang, Feiyi},
	date = {2019-11},
	keywords = {Large-scale parallel filesystem, machine learning, production supercomputer, write performance},
}

@article{zhao_optimizing_2018,
	title = {Optimizing Convolutional Neural Networks on the Sunway {TaihuLight} Supercomputer},
	volume = {15},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/3177885},
	doi = {10.1145/3177885},
	abstract = {The Sunway {TaihuLight} supercomputer is powered by {SW}26010, a new 260-core processor designed with on-chip fusion of heterogeneous cores. In this article, we present our work on optimizing the training process of convolutional neural networks ({CNNs}) on the Sunway {TaihuLight} supercomputer. Specifically, a highly efficient library ({swDNN}) and a customized Caffe framework ({swCaffe}) are proposed. Architecture-oriented optimization methods targeting the many-core architecture of {SW}26010 are introduced and are able to achieve 48× speedup for the convolution routine in {swDNN} and 4× speedup for the complete training process of the {VGG}-16 network using {swCaffe}, compared to the unoptimized algorithm and framework. Compared to the {cuDNN} library and the Caffe framework based on the {NVIDIA} K40m {GPU}, the proposed {swDNN} library and {swCaffe} framework on {SW}26010 have nearly half the performance of K40m in single -precision and have 3.6× and 1.8× speedup over K40m in double precision, respectively.},
	pages = {13:1--13:26},
	number = {1},
	journaltitle = {{ACM} Transactions on Architecture and Code Optimization},
	shortjournal = {{ACM} Trans. Archit. Code Optim.},
	author = {Zhao, Wenlai and Fu, Haohuan and Fang, Jiarui and Zheng, Weijie and Gan, Lin and Yang, Guangwen},
	urldate = {2022-03-13},
	date = {2018-03-22},
	keywords = {Convolutional neural network, Sunway {TaihuLight} supercomputer, deep learning, heterogeneous many-core architecture},
}

@article{li_swflow_2021,
	title = {{swFLOW}: A large-scale distributed framework for deep learning on Sunway {TaihuLight} supercomputer},
	volume = {570},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520312457},
	doi = {10.1016/j.ins.2020.12.079},
	shorttitle = {{swFLOW}},
	abstract = {Deep learning technology is widely used in many modern fields and a number of models and software frameworks have been proposed. However, it is still very difficult to process deep learning tasks efficiently on traditional high performance computing ({HPC}) systems. In this paper, we propose {swFLOW}: a large-scale distributed framework for deep learning on Sunway {TaihuLight}. Based on the performance analysis results of convolutional neural network ({CNN}), we optimize the convolutional layer, and get 10.42× speedup compared to the original version. As for distributed training, we use elastic averaging stochastic gradient descent ({EASGD}) algorithm to reduce communication. On 512 processes, we get a parallel efficiency of 81.01\% with communication period τ=8. Particularly, a decentralized implementation of distributed {swFLOW} system is presented to alleviate bottleneck of the central server. By using distributed {swFLOW} system, we can scale the batch size up to 4096 among 1024 concurrent processes for cancerous region detection algorithm. The successful application on {swFLOW} reveals the great opportunity for joint combination of deep learning and {HPC} system.},
	pages = {831--847},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Li, Mingfan and Lin, Han and Chen, Junshi and Diaz, Jose Monsalve and Xiao, Qian and Lin, Rongfen and Wang, Fei and Gao, Guang R. and An, Hong},
	urldate = {2022-03-13},
	date = {2021-09-01},
	langid = {english},
	keywords = {Cancerous region detection, Convolutional neural networks, Deep learning, High performance computing},
}

@inproceedings{yin_strategies_2019,
	title = {Strategies to Deploy and Scale Deep Learning on the Summit Supercomputer},
	doi = {10.1109/DLS49591.2019.00016},
	abstract = {The rapid growth and wide applicability of Deep Learning ({DL}) frameworks poses challenges to computing centers which need to deploy and support the software, and also to domain scientists who have to keep up with the system environment and scale up scientific exploration through {DL}. We offer recommendations for deploying and scaling {DL} frameworks on the Summit supercomputer, currently atop the Top500 list, at the Oak Ridge National Laboratory Leadership Computing Facility ({OLCF}). We discuss {DL} software deployment in the form of containers, and compare performance of native-built frameworks and containerized deployment. Software containers show no noticeable negative performance impact and exhibit faster Python loading times and promise easier maintenance. To explore strategies for scaling up {DL} model training campaigns, we assess {DL} compute kernel performance, discuss and recommend I/O data formats and staging, and identify communication needs for scalable message exchange for {DL} runs at scale. We recommend that users take a step-wise tuning approach beginning with algorithmic kernel choice, node I/O configuration, and communications tuning as best-practice. We present baseline examples of scaling efficiency 87\% for a {DL} run of {ResNet}50 running on 1024 nodes (6144 V100 {GPUs}).},
	eventtitle = {2019 {IEEE}/{ACM} Third Workshop on Deep Learning on Supercomputers ({DLS})},
	pages = {84--94},
	booktitle = {2019 {IEEE}/{ACM} Third Workshop on Deep Learning on Supercomputers ({DLS})},
	author = {Yin, Junqi and Gahlot, Shubhankar and Laanait, Nouamane and Maheshwari, Ketan and Morrison, Jack and Dash, Sajal and Shankar, Mallikarjun},
	date = {2019-11},
	keywords = {Containers, Deep learning, Graphics processing units, Libraries, Supercomputers, Tuning},
}

@article{hamilton_inductive_2018,
	title = {Inductive Representation Learning on Large Graphs},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present {GraphSAGE}, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	journaltitle = {{arXiv}:1706.02216 [cs, stat]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2022-03-11},
	date = {2018-09-10},
	eprinttype = {arxiv},
	eprint = {1706.02216},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2022-03-11},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{oono_asymptotic_2019,
	title = {On Asymptotic Behaviors of Graph {CNNs} from Dynamical Systems Perspective},
	url = {https://openreview.net/forum?id=mPLlSuOPxJh},
	abstract = {Graph Neural Networks (graph {NNs}) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive...},
	author = {Oono, Kenta and Suzuki, Taiji},
	urldate = {2022-03-11},
	date = {2019-01-01},
	langid = {english},
}

@article{klicpera_predict_2019,
	title = {Predict then Propagate: Graph Neural Networks meet Personalized {PageRank}},
	url = {http://arxiv.org/abs/1810.05997},
	shorttitle = {Predict then Propagate},
	abstract = {Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks ({GCN}) and {PageRank} to derive an improved propagation scheme based on personalized {PageRank}. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions ({PPNP}), and its fast approximation, {APPNP}. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for {GCN}-like models. Our implementation is available online.},
	journaltitle = {{arXiv}:1810.05997 [cs, stat]},
	author = {Klicpera, Johannes and Bojchevski, Aleksandar and Günnemann, Stephan},
	urldate = {2022-03-11},
	date = {2019-02-27},
	eprinttype = {arxiv},
	eprint = {1810.05997},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{velickovic_graph_2018,
	title = {Graph Attention Networks},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks ({GATs}), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our {GAT} models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	journaltitle = {{arXiv}:1710.10903 [cs, stat]},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	urldate = {2022-03-11},
	date = {2018-02-04},
	eprinttype = {arxiv},
	eprint = {1710.10903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{li_deepgcns_2019,
	title = {{DeepGCNs}: Can {GCNs} Go as Deep as {CNNs}?},
	url = {http://arxiv.org/abs/1904.03751},
	shorttitle = {{DeepGCNs}},
	abstract = {Convolutional Neural Networks ({CNNs}) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep {CNN} models were able to be reliably trained. Despite their merits, {CNNs} fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks ({GCNs}) build graphs to represent non-Euclidean data, borrow concepts from {CNNs}, and apply them in training. {GCNs} show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art {GCN} models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep {GCNs}. We do this by borrowing concepts from {CNNs}, specifically residual/dense connections and dilated convolutions, and adapting them to {GCN} architectures. Extensive experiments show the positive effect of these deep {GCN} frameworks. Finally, we use these new concepts to build a very deep 56-layer {GCN}, and show how it significantly boosts performance (+3.7\% {mIoU} over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing {GCN}-based research.},
	journaltitle = {{arXiv}:1904.03751 [cs]},
	author = {Li, Guohao and Müller, Matthias and Thabet, Ali and Ghanem, Bernard},
	urldate = {2022-03-11},
	date = {2019-08-19},
	eprinttype = {arxiv},
	eprint = {1904.03751},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{xu_representation_2018,
	title = {Representation Learning on Graphs with Jumping Knowledge Networks},
	url = {http://arxiv.org/abs/1806.03536},
	abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge ({JK}) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the {JK} framework with models like Graph Convolutional Networks, {GraphSAGE} and Graph Attention Networks consistently improves those models' performance.},
	journaltitle = {{arXiv}:1806.03536 [cs, stat]},
	author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	urldate = {2022-03-11},
	date = {2018-06-25},
	eprinttype = {arxiv},
	eprint = {1806.03536},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_deeper_2018,
	title = {Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning},
	url = {http://arxiv.org/abs/1801.07606},
	abstract = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks ({GCNs}), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the {GCN} model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires a considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the {GCN} model and address its fundamental limits. First, we show that the graph convolution of the {GCN} model is actually a special form of Laplacian smoothing, which is the key reason why {GCNs} work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the {GCN} model with shallow architectures, we propose both co-training and self-training approaches to train {GCNs}. Our approaches significantly improve {GCNs} in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
	journaltitle = {{arXiv}:1801.07606 [cs, stat]},
	author = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
	urldate = {2022-03-11},
	date = {2018-01-22},
	eprinttype = {arxiv},
	eprint = {1801.07606},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kipf_semi-supervised_2017,
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	journaltitle = {{arXiv}:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	urldate = {2022-03-11},
	date = {2017-02-22},
	eprinttype = {arxiv},
	eprint = {1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rong_dropedge_2020,
	title = {{DropEdge}: Towards Deep Graph Convolutional Networks on Node Classification},
	url = {http://arxiv.org/abs/1907.10903},
	shorttitle = {{DropEdge}},
	abstract = {{\textbackslash}emph\{Over-fitting\} and {\textbackslash}emph\{over-smoothing\} are two main obstacles of developing deep Graph Convolutional Networks ({GCNs}) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes {DropEdge}, a novel and flexible technique to alleviate both issues. At its core, {DropEdge} randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that {DropEdge} either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our {DropEdge} is a general skill that can be equipped with many other backbone models (e.g. {GCN}, {ResGCN}, {GraphSAGE}, and {JKNet}) for enhanced performance. Extensive experiments on several benchmarks verify that {DropEdge} consistently improves the performance on a variety of both shallow and deep {GCNs}. The effect of {DropEdge} on preventing over-smoothing is empirically visualized and validated as well. Codes are released on{\textasciitilde}{\textbackslash}url\{https://github.com/{DropEdge}/{DropEdge}\}.},
	journaltitle = {{arXiv}:1907.10903 [cs, stat]},
	author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
	urldate = {2022-03-11},
	date = {2020-03-12},
	eprinttype = {arxiv},
	eprint = {1907.10903},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Statistics - Machine Learning},
}

@online{noauthor_zotero_nodate,
	title = {Zotero {\textbar} Settings {\textgreater} Feeds/{API} {\textgreater} New Key},
	url = {https://www.zotero.org/settings/keys/new},
	urldate = {2022-03-11},
}